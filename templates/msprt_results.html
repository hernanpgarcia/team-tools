{% extends "base.html" %}

{% block title %}mSPRT Sequential Testing Results{% endblock %}

{% block content %}
<h2>mSPRT Sequential Testing Results</h2>

<div class="results-grid">
    <div class="results-section">
        <h3>üìä Test Configuration</h3>
        <table class="results-table">
            <tr><td><strong>Baseline Mean:</strong></td><td>{{ baseline_mean }}</td></tr>
            <tr><td><strong>Standard Deviation:</strong></td><td>{{ "%.3f"|format(baseline_std) }} ({{ std_method }})</td></tr>
            <tr><td><strong>Expected Improvement:</strong></td><td>{{ "%+.3f"|format(absolute_improvement) }} ({{ "%+.1f%%"|format(relative_improvement) }})</td></tr>
            <tr><td><strong>Effect Size (Cohen's d):</strong></td><td>{{ "%.3f"|format(effect_size) }}</td></tr>
            <tr><td><strong>Test Method:</strong></td><td>{{ "Welch's t-test" if use_t_test else "Z-test" }}</td></tr>
        </table>
    </div>

    <div class="results-section">
        <h3>üìà mSPRT Parameters</h3>
        <table class="results-table">
            <tr><td><strong>Type I Error (Œ±):</strong></td><td>{{ "%.1f%%"|format(alpha*100) }}</td></tr>
            <tr><td><strong>Type II Error (Œ≤):</strong></td><td>{{ "%.1f%%"|format(beta*100) }}</td></tr>
            <tr><td><strong>Power:</strong></td><td>{{ "%.1f%%"|format(power*100) }}</td></tr>
            <tr><td><strong>Upper Threshold (A):</strong></td><td>{{ "%.2f"|format(A) }}</td></tr>
            <tr><td><strong>Lower Threshold (B):</strong></td><td>{{ "%.4f"|format(B) }}</td></tr>
        </table>
    </div>

    <div class="results-section highlight">
        <h3>üéØ Expected Timeline</h3>
        {% if weekly_visitors %}
        <div class="expected-performance">
            <div class="performance-item">
                <div class="performance-label"><strong>Average case</strong><br>(Full improvement)</div>
                <div class="performance-value">~{{ "%.0f"|format(expected_n_h1 / weekly_visitors) }} weeks</div>
            </div>
            <div class="performance-item">
                <div class="performance-label"><strong>Worst case</strong><br>(50% improvement)</div>
                <div class="performance-value">~{{ "%.0f"|format(expected_n_half_effect / weekly_visitors) }} weeks</div>
            </div>
        </div>
        <div class="efficiency-highlight">
            <strong>Note:</strong> This is the theoretical average. The actual decision week depends on your real data results.
        </div>
        {% else %}
        <div class="expected-performance">
            <div class="performance-item">
                <div class="performance-label">If no effect exists (H‚ÇÄ)</div>
                <div class="performance-value">~{{ "%.0f"|format(expected_n_h0) }} samples/group</div>
            </div>
            <div class="performance-item">
                <div class="performance-label">If effect exists (H‚ÇÅ)</div>
                <div class="performance-value">~{{ "%.0f"|format(expected_n_h1) }} samples/group</div>
            </div>
        </div>
        <div class="efficiency-highlight">
            <strong>Efficiency Gain:</strong> Up to {{ "%.0f%%"|format(efficiency_gain) }} reduction in sample size when effect exists
        </div>
        {% endif %}
    </div>
</div>

{% if weekly_visitors %}
<div class="monitoring-section">
    <h3>üìÖ Weekly Monitoring Plan</h3>
    <p class="monitoring-description">
        With <strong>{{ weekly_visitors }}</strong> visitors per group per week, here's when you can make decisions based on your actual test results:
    </p>
    
    <div class="table-container">
        <table class="weekly-monitoring-table">
            <thead>
                <tr>
                    <th>Week</th>
                    <th>Sample Size</th>
                    <th>Min Detectable Effect</th>
                    <th>Decision Status</th>
                    <th>What This Means</th>
                </tr>
            </thead>
            <tbody>
                {% for point in monitoring_points %}
                <tr class="{% if point.status == '‚úÖ Significant Improvement' %}success{% elif point.status == '‚ùå Significant Decline' %}danger{% else %}warning{% endif %}">
                    <td><strong>{{ point.week }}</strong></td>
                    <td>{{ "%.0f"|format(point.n) }}</td>
                    <td>{{ "%.1f%%"|format((point.boundary_upper / baseline_mean) * 100) }}</td>
                    <td>{{ point.status }}</td>
                    <td>{{ point.explanation }}</td>
                </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
    
    <div class="monitoring-legend">
        <h4>üìñ How to Read This Table:</h4>
        <ul>
            <li><strong>Min Detectable Effect:</strong> The smallest improvement we can reliably detect at this week</li>
            <li><strong>‚úÖ Significant Improvement:</strong> You can stop the test and implement the change</li>
            <li><strong>‚ùå Significant Decline:</strong> You can stop the test and keep the current version</li>
            <li><strong>‚è≥ Keep Testing:</strong> Results are not clear yet, continue to next week</li>
        </ul>
        
        <div class="timeline-explanation">
            <h4>üìä Expected vs. Actual Timeline:</h4>
            <p><strong>Expected Timeline</strong> shows the theoretical average when an effect exists.<br>
            <strong>Weekly Monitoring</strong> shows the earliest possible decision points based on statistical power.<br>
            Your actual results may enable earlier or later decisions depending on the data.</p>
        </div>
    </div>
</div>
{% else %}
<div class="monitoring-section">
    <h3>üìä Sequential Monitoring Plan</h3>
    <div class="table-container">
        <table class="monitoring-table">
            <thead>
                <tr>
                    <th>Sample Size/Group</th>
                    <th>Decision Boundary</th>
                    <th>95% CI (Absolute)</th>
                    <th>95% CI (Relative)</th>
                    <th>Standard Error</th>
                </tr>
            </thead>
            <tbody>
                {% for point in monitoring_points %}
                <tr>
                    <td>{{ "%.0f"|format(point.n) }}</td>
                    <td>¬±{{ "%.4f"|format(point.boundary_upper) }}</td>
                    <td>[{{ "%+.3f"|format(point.ci_lower) }}, {{ "%+.3f"|format(point.ci_upper) }}]</td>
                    <td>[{{ "%+.1f%%"|format(point.rel_ci_lower) }}, {{ "%+.1f%%"|format(point.rel_ci_upper) }}]</td>
                    <td>{{ "%.4f"|format(point.se) }}</td>
                </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
</div>
{% endif %}

<div class="implementation-guide">
    <h3>üìã mSPRT Implementation Guide</h3>
    <div class="guide-steps">
        <div class="step">
            <div class="step-number">1</div>
            <div class="step-content">
                <strong>Collect minimum samples:</strong> Wait until you have at least {{ "%.0f"|format(min_n) }} samples per group
            </div>
        </div>
        <div class="step">
            <div class="step-number">2</div>
            <div class="step-content">
                <strong>Calculate test statistic:</strong> Difference in means divided by pooled standard error
            </div>
        </div>
        <div class="step">
            <div class="step-number">3</div>
            <div class="step-content">
                <strong>Decision rules:</strong>
                <ul>
                    <li><strong>Stop and declare significance:</strong> If observed difference > upper boundary</li>
                    <li><strong>Stop and declare no effect:</strong> If observed difference < lower boundary</li>
                    <li><strong>Continue testing:</strong> If difference is within boundaries</li>
                </ul>
            </div>
        </div>
        <div class="step">
            <div class="step-number">4</div>
            <div class="step-content">
                <strong>Safety cap:</strong> Stop at {{ "%.0f"|format(max_n) }} samples per group maximum
            </div>
        </div>
    </div>
</div>

<div class="advantages-section">
    <h3>üí° mSPRT Advantages</h3>
    <div class="advantages-list">
        <div class="advantage">
            <strong>Always valid:</strong> Type I error rate exactly controlled at {{ "%.1f%%"|format(alpha*100) }}
        </div>
        <div class="advantage">
            <strong>Continuous monitoring:</strong> Check results at any sample size ‚â• {{ "%.0f"|format(min_n) }}
        </div>
        <div class="advantage">
            <strong>Optimal efficiency:</strong> Minimizes expected sample size
        </div>
        <div class="advantage">
            <strong>Robust to variance:</strong> {{ std_method }}
        </div>
    </div>
</div>

<div class="important-notes">
    <h3>‚ö†Ô∏è Important Notes</h3>
    <ul>
        <li><strong>Stick to the plan:</strong> Only stop when boundaries are crossed</li>
        <li><strong>No peeking penalty:</strong> mSPRT allows continuous monitoring without alpha inflation</li>
        {% if use_t_test %}
        <li><strong>Variance estimation:</strong> Update variance estimates periodically for better accuracy</li>
        {% endif %}
    </ul>
</div>

<div class="navigation-links">
    <a href="/sequential-calculator">‚Üê Calculate Another mSPRT Test</a>
    <a href="/sample-size-calculator">‚Üí Try Fixed Horizon Testing</a>
    <a href="/">‚Üê Back to Dashboard</a>
</div>
{% endblock %}
